<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>FALCON</title>

  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/UA_Logo.png">

  <!-- Fonts and Icons -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">

  <!-- CSS Libraries -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  <!-- Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-825ET94Y3E');
  </script>

  <!-- JavaScript Libraries -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              FALCON <img src="static/images/falcon-icon.png" alt="Icon" style="height: 52px;">: Fairness Learning via
              Contrastive Attention Approach to Continual Semantic Scene Understanding
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://truongthanhdat.github.io/" target="_blank">Thanh-Dat Truong<sup>1</sup></a>,</span>
              <a href="https://scholar.google.com/citations?user=o9GWp-YAAAAJ&hl=en" target="_blank">Usav
                Prabhu<sup>2</sup></a>,</span>
              <a href="https://www.ece.cmu.edu/directory/bios/raj-bhiksha.html" target="_blank">Bhiksha
                Raj<sup>3,4</sup></a>,</span>
              <a href="https://cast.uark.edu/directory/index/uid/jcothre/name/Jackson+David+Cothren/"
                target="_blank">Jackson Cothren<sup>5</sup></a></span>
              <a href="https://engineering.uark.edu/electrical-engineering-computer-science/electrical-engineering-faculty/uid/khoaluu/name/Khoa+Luu/"
                target="_blank">Khoa Luu<sup>1</sup></a></span>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>CVIU Lab, University of Arkansas&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>2</sup>Google DeepMind&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>3</sup>CMU&nbsp;&nbsp;&nbsp;&nbsp;<br>
                <sup>4</sup>MBZUAI&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>5</sup>GEOS Department, University of Arkansas&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
              <br>
            </div>
            <p style="color:#000; font-weight:bold; font-size:1.5em;" class="text-center">ðŸŽ‰ <strong>Accepted to CVPR 2025</strong> ðŸŽ‰</p>
            <!-- Publication Links -->
            <div class="publication-links">
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2311.15965" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/uark-cviu/FALCON/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

            </div>
            <!-- Video Container -->
            <div class="video-container">
              <video autoplay muted loop playsinline>
                <source src="static/videos/teaser-v2.mp4" type="video/mp4">
              </video>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper motivation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Highlights</h2>
          <div class="content has-text-justified">
            <p>
            <ul>
              <li><strong>Contrastive Clustering Paradigm for Continual Learning.</strong> Introduce a novel approach to
                addressing catastrophic forgetting in continual semantic segmentation by leveraging contrastive
                clustering.</li>

              <li><strong>Fairness Contrastive Clustering Loss.</strong> Develop a fairness-driven contrastive loss to
                mitigate bias in continual learning, improving model fairness on imbalanced data.</li>


              <li><strong>Attention-based Visual Grammar.</strong> Propose a new attention-based framework to model
                feature distributions and topological structures, effectively handling background shift and unknown
                classes.</li>
            </ul>
            <figure>
              <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
              <img src="static/images/FirstFig-V3.jpg" alt="fail" width="100%">
            </figure>
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Continual Learning in semantic scene segmentation aims to continually learn new unseen classes in dynamic
              environments while maintaining previously learned knowledge. Prior studies focused on modeling the
              catastrophic forgetting and background shift challenges in continual learning. However, fairness, another
              major challenge that causes unfair predictions leading to low performance among major and minor classes,
              still needs to be well addressed. In addition, prior methods have yet to model the unknown classes well,
              thus resulting in producing non-discriminative features among unknown classes. This paper presents a novel
              Fairness Learning via Contrastive Attention Approach to continual learning in semantic scene
              understanding. In particular, we first introduce a new Fairness Contrastive Clustering loss to address the
              problems of catastrophic forgetting and fairness. Then, we propose an attention-based visual grammar
              approach to effectively model the background shift problem and unknown classes, producing better feature
              representations for different unknown classes. Through our experiments, our proposed approach achieves
              State-of-the-Art (SOTA) performance on different continual learning benchmarks, i.e., ADE20K, Cityscapes,
              and Pascal VOC. It promotes the fairness of the continual semantic segmentation model.
            </p>
            <figure>
              <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
              <img src="static/images/DraftFramework-V3.jpg" alt="fail" width="100%">
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>ADE20K Benchmarks</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/ADE20K.png" alt="fail" width="100%">
          </figure>

          <br>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>Pascal VOC Benchmarks</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/VOC.png" alt="fail" width="100%">
          </figure>

          <br>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>Cityscapes Benchmarks</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/Cityscapes.png" alt="fail" width="50%">
          </figure>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>Qualitative Results</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/visualization.jpg" alt="fail" width="100%">
          </figure>


        </div>
      </div>
    </div>
  </section>

  <section class="BibTeX">
    <div class="container is-max-desktop" align="justify">
      <div align="center"><h2 class="title is-3">Our Related Work</h2></div>
        <br>
  
        [1] Thanh-Dat Truong, Chi Nhan Duong, Ngan Le, Son Lam Phung, Chase Rainwater, and Khoa Luu. 
  <strong>"BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation."</strong> 
        In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 8548-8557. 2021.
          
        <br><br>
  
        [2]Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson Cothren, and Khoa Luu. 
      <strong>"FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding."</strong>
        In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19988-19997. 2023.
  
        <br><br>
  
        [3] Thanh-Dat Truong, Hoang-Quan Nguyen, Bhiksha Raj, and Khoa Luu.
      <strong>"Fairness Continual Learning Approach to Semantic Scene Understanding in Open-world Environments."</strong>
        Advances in Neural Information Processing Systems (NeurIPS), pp. 65456-65467. 2023.
  
        <br><br>
  
        [4] Thanh-Dat Truong, Pierce Helton, Ahmed Moustafa, Jackson David Cothren, and Khoa Luu. 
        <strong>"CONDA: Continual Unsupervised Domain Adaptation Learning in Visual Perception for Self-driving Cars."</strong> 
        In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 5642-5650. 2024.
  
        <br><br>
  
        [5] Thanh-Dat Truong, Utsav Prabhu, Bhiksha Raj, Jackson Cothren, and Khoa Luu. 
      <strong>"FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding."</strong> 
        In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2025.      
  
  </div>
  </section>

  


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div align="center">
        <h2 class="title is-3">BibTex</h2>
      </div>
      <pre><code>
      @inproceedings{truong2023falcon,
        title={FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding},
        author={Truong, Thanh-Dat and Prabhu, Utsav and Raj, Bhiksha and Cothren, Jackson and Luu, Khoa},
        booktitle={The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        year={2025}
      }
    </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <center>
              <p>
                This project template was adopted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>.
              </p>
            </center>

          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- Initialize Slick Carousel -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      let currentVideoIndex = 0;
      const videos = document.querySelectorAll('.videos video');
      const totalVideos = videos.length;

      document.getElementById('prevBtn').addEventListener('click', function () {
        changeVideo(-1);
      });

      document.getElementById('nextBtn').addEventListener('click', function () {
        changeVideo(1);
      });

      document.querySelectorAll('.img-zoom').forEach(img => {
        img.addEventListener('click', function () {
          this.classList.toggle('active');
        });
      });

      function changeVideo(direction) {
        videos[currentVideoIndex].style.display = 'none';
        currentVideoIndex = (currentVideoIndex + direction + totalVideos) % totalVideos;
        videos[currentVideoIndex].style.display = 'block';
      }

      function openModal(img) {
        var modal = document.getElementById("myModal");
        var modalImg = document.getElementById("img01");
        modal.style.display = "block";
        modalImg.src = img.src;
      }

      var span = document.getElementsByClassName("close")[0];
      if (span) {
        span.onclick = function () {
          var modal = document.getElementById("myModal");
          modal.style.display = "none";
        }
      }
    });
  </script>
</body>

</html>