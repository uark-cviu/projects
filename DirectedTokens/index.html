<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DirectedTokens</title>

  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/UA_Logo.png">

  <!-- Fonts and Icons -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">

  <!-- CSS Libraries -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  <!-- Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-825ET94Y3E');
  </script>

  <!-- JavaScript Libraries -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Directed-Tokens: A Robust Multi-Modality Alignment Approach to
              Large Language-Vision Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Thanh-Dat Truong<sup>1</sup></a>,</span>
              <a href="" target="_blank">Huu-Thien Tran<sup>1</sup></a>,</span>
              <a href="" target="_blank">Thai Son Tran<sup>2</sup></a>,</span>
              <a href="" target="_blank">Bhiksha Raj<sup>3,4</sup></a>,</span>
              <a href="" target="_blank">Khoa Luu<sup>1</sup></a></span>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>CVIU Lab, University of Arkansas&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>2</sup>University of Science, VNU-HCM&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>3</sup>Carnegie Mellon University&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>4</sup>Mohammed bin Zayed University of AI, UAE&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
              <br>
            </div>
            <p style="color:#000; font-weight:bold; font-size:1.5em;" class="text-center">ðŸŽ‰ <strong>Accepted to NeurIPS
                2025</strong> ðŸŽ‰</p>
            <!-- Publication Links -->
            <div class="publication-links">
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2508.14264" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>

              </span>
              <span class="link-block">
                <a href="https://github.com/uark-cviu/DirectedTokens" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                </a>
              </span>

            </div>

            <figure>
              <img src="static/images/robust-llava-framework-v13.jpg" alt="fail" width="100%">
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper motivation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Highlights</h2>
          <div class="content has-text-justified">
            <p>
            <ul>
              <li>Shuffling-based Alignment Tasks: We introduce two novel pre-training and fine-tuning tasksâ€”image order
                reconstruction and text order reconstructionâ€”that strengthen reasoning, visual understanding, and
                cross-modality alignment in LMMs.</li>

              <li>Directed-Token Representation: We propose a directed-token mechanism that effectively captures visual
                and textual knowledge, enabling the model to reconstruct correct visual sequences and enhance robust
                alignment.</lie>

              <li>Image-to-Response Guided Loss: We design a new loss function that explicitly guides responses with
                visual understanding, leading to consistent state-of-the-art performance on academic benchmarks for
                task-oriented and instruction-following LMMs.</li>
            </ul>
            <figure>
              <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
              <img src="static/images/direct-token-llava.jpg" alt="fail" width="100%">
            </figure>
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large multimodal models (LMMs) have gained impressive performance due to their outstanding capability in
              various
              understanding tasks. However, these models still suffer from some fundamental limitations related to
              robustness and
              generalization due to the alignment and correlation between visual and textual features. In this paper, we
              introduce a
              simple but efficient learning mechanism for improving the robust alignment between visual and textual
              modalities by
              solving shuffling problems. In particular, the proposed approach can improve reasoning capability, visual
              understanding,
              and cross-modality alignment by introducing two new tasks: reconstructing the image order and the text
              order into the
              LMM's pre-training and fine-tuning phases. In addition, we propose a new directed-token approach to
              capture visual and
              textual knowledge, enabling the capability to reconstruct the correct order of visual inputs. Then, we
              introduce a new
              Image-to-Response Guided loss to further improve the visual understanding of the LMM in its responses. The
              proposed
              approach consistently achieves state-of-the-art (SoTA) performance compared with prior LMMs on academic
              task-oriented
              and instruction-following LMM benchmarks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Results</h2>

          <figure>
            <figcaption class="content has-text-left" style="word-break:normal">Comparison of Response in Conversation
              Between Direct-LLaVA-7B and LLaVA-v1.5-7B on In-the-Wild Samples.
              <img src="static/images/comparision-in-the-wild.jpg" alt="fail" width="100%">
          </figure>

          <figure>
            <figcaption class="content has-text-left" style="word-break:normal">Comparison of Response in Multiple-
              Choice Questions Between Direct-LLaVA-7B and LLaVA-v1.5-7B on MMMU-Val.
              <img src="static/images/comparision-mmmu.jpg" alt="fail" width="100%">
          </figure>

        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>Comparison with Prior Methods on Academic-task-oriented Benchmarks</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/academic-results.png" alt="fail" width="100%">
          </figure>

          <br>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>Comparison with Prior Methods on Benchmarks for Instruction-Following LMMs</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/instruction-following-results.png" alt="fail" width="100%">
          </figure>





        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Acknowledgements</h2>
      <p>This work is partly supported by NSF CAREER (No. 2442295), NSF SCH (No. 2501021), NSF E-RISE (No. 2445877), NSF SBIR Phase 2 (No. 2247237) and USDA/NIFA Award. We also acknowledge the Arkansas High-Performance Computing Center (HPC) for GPU servers.</p>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">BibTex</h2>
      <pre><code>@inproceedings{truong2025directedtokens,
      title={Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models},
      author={Thanh-Dat Truong and Huu-Thien Tran and Thai Son Tran and Bhiksha Raj and Khoa Luu},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
      year={2025}
    }
    </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <center>
              <p>
                This project template was adopted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>.
              </p>
            </center>

          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- Initialize Slick Carousel -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      let currentVideoIndex = 0;
      const videos = document.querySelectorAll('.videos video');
      const totalVideos = videos.length;

      document.getElementById('prevBtn').addEventListener('click', function () {
        changeVideo(-1);
      });

      document.getElementById('nextBtn').addEventListener('click', function () {
        changeVideo(1);
      });

      document.querySelectorAll('.img-zoom').forEach(img => {
        img.addEventListener('click', function () {
          this.classList.toggle('active');
        });
      });

      function changeVideo(direction) {
        videos[currentVideoIndex].style.display = 'none';
        currentVideoIndex = (currentVideoIndex + direction + totalVideos) % totalVideos;
        videos[currentVideoIndex].style.display = 'block';
      }

      function openModal(img) {
        var modal = document.getElementById("myModal");
        var modalImg = document.getElementById("img01");
        modal.style.display = "block";
        modalImg.src = img.src;
      }

      var span = document.getElementsByClassName("close")[0];
      if (span) {
        span.onclick = function () {
          var modal = document.getElementById("myModal");
          modal.style.display = "none";
        }
      }
    });
  </script>
</body>

</html>