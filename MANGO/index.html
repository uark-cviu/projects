<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MANGO</title>

  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/UA_Logo.png">

  <!-- Fonts and Icons -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">

  <!-- CSS Libraries -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

  <!-- Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-825ET94Y3E');
  </script>

  <!-- JavaScript Libraries -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MANGO: Multimodal Attention-based Normalizing Flow Approach to
              Fusion Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Thanh-Dat Truong<sup>1</sup></a>,</span>
              <a href="" target="_blank">Christophe Bobda<sup>2</sup></a>,</span>
              <a href="" target="_blank">Nitin Agarwal<sup>3</sup></a>,</span>
              <a href="" target="_blank">Khoa Luu<sup>1</sup></a></span>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>CVIU Lab, University of Arkansas&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>2</sup>University of Florida&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>3</sup>University of Arkansas at Little Rock&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
              <br>
            </div>
            <p style="color:#000; font-weight:bold; font-size:1.5em;" class="text-center">ðŸŽ‰ <strong>Accepted to NeurIPS
                2025</strong> ðŸŽ‰</p>
            <!-- Publication Links -->
            <div class="publication-links">
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2508.10133" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

            </div>

            <figure>
              <img src="static/images/IdeaFusionNVP-v7.jpg" alt="fail" width="100%">
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper motivation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Highlights</h2>
          <div class="content has-text-justified">
            <p>
            <ul>
              <li>Invertible Cross-Attention Layer: We propose a novel Invertible Cross-Attention (ICA) layer that
                integrates with normalizing flows to provide explicit, interpretable, and tractable multimodal fusion.
              </li>

              <li>DNew Cross-Attention Mechanisms: We design three complementary mechanismsâ€”Modality-to-Modality
                Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
                Cross-Attention (LICA)â€”to efficiently capture complex inter- and intra-modality correlations.</lie>

              <li>Scalable Multimodal Normalizing Flow: We introduce the Multimodal Attention-based Normalizing Flow
                (MANGO), enabling scalable and effective modeling of high-dimensional multimodal data, achieving
                state-of-the-art performance across diverse tasks including semantic segmentation, image-to-image
                translation, and movie genre classification.</li>
            </ul>
            <figure>
              <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
              <img src="static/images/forward-inverse-attention.jpg" alt="fail" width="100%">
            </figure>
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Multimodal learning has gained much success in recent years. However, current multimodal fusion methods
              adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal
              features. As a result, the multimodal model cannot capture the essential features of each modality, making
              it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces
              a novel Multimodal Attention-based Normalizing Flow (MANGO) approach\footnote{The source code of this work
              will be publicly available.} to developing explicit, interpretable, and tractable multimodal fusion
              learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the
              Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying
              correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new
              cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention
              (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal
              Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional
              multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic
              segmentation, image-to-image translation, and movie genre classification, have illustrated the
              state-of-the-art (SoTA) performance of the proposed approach.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Results</h2>

          <figure>
            <figcaption class="content has-text-left" style="word-break:normal">Qualitative Comparison on NYUDv2 Benchmark.
              <img src="static/images/semantic-segmentation-v2.jpg" alt="fail" width="100%">
          </figure>

          <figure>
            <figcaption class="content has-text-left" style="word-break:normal">Qualitative Comparison on Image-to-Image Benchmark.
              <img src="static/images/image-translation-v2.jpg" alt="fail" width="100%">
          </figure>

        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>Comparison of RGB-D Semantic Segmentation Performance on NYUDv2 and SUN RGB-D with Prior Methods.</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/seg-results.png" alt="fail" width="100%">
          </figure>

          <br>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>Comparison of Multimodal Image Translation Performance on Taskonomy with Prior Multimodal Methods.</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/i2i-results.png" alt="fail" width="100%">
          </figure>

          <div class="content has-text-left is-size-5">
            <center>
              <strong>omparison of Movie Genre Classification Performance on the MM-IMDB dataset with Prior Multimodal Methods.</strong>
            </center>
          </div>

          <figure>
            <!-- <figcaption class="content has-text-left" style="word-break:normal"> Comparisons with Domain Adaptation Approaches. -->
            <img src="static/images/music-results.png" alt="fail" width="100%">
          </figure>





        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">BibTex</h2>
      <pre><code>@inproceedings{truong2025mango,
      title={MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning},
      author={Thanh-Dat Truong and Christophe Bobda and Nitin Agarwal and Khoa Luu},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
      year={2025}
    }
    </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <center>
              <p>
                This project template was adopted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>.
              </p>
            </center>

          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- Initialize Slick Carousel -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      let currentVideoIndex = 0;
      const videos = document.querySelectorAll('.videos video');
      const totalVideos = videos.length;

      document.getElementById('prevBtn').addEventListener('click', function () {
        changeVideo(-1);
      });

      document.getElementById('nextBtn').addEventListener('click', function () {
        changeVideo(1);
      });

      document.querySelectorAll('.img-zoom').forEach(img => {
        img.addEventListener('click', function () {
          this.classList.toggle('active');
        });
      });

      function changeVideo(direction) {
        videos[currentVideoIndex].style.display = 'none';
        currentVideoIndex = (currentVideoIndex + direction + totalVideos) % totalVideos;
        videos[currentVideoIndex].style.display = 'block';
      }

      function openModal(img) {
        var modal = document.getElementById("myModal");
        var modalImg = document.getElementById("img01");
        modal.style.display = "block";
        modalImg.src = img.src;
      }

      var span = document.getElementsByClassName("close")[0];
      if (span) {
        span.onclick = function () {
          var modal = document.getElementById("myModal");
          modal.style.display = "none";
        }
      }
    });
  </script>
</body>

</html>